{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimso\\anaconda3\\envs\\test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_final = pd.read_csv('./dataset/novel_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = novel_final.copy()\n",
    "data = data[:int(len(data)*0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>top_sentiment</th>\n",
       "      <th>second_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>이야기꾼 구연설화</td>\n",
       "      <td>['01범보다 무서운 곶감', '화자를 처음 만나 이야기를 들으러 왔다고 하자 서슴...</td>\n",
       "      <td>분노</td>\n",
       "      <td>당황</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>이야기꾼 구연설화</td>\n",
       "      <td>['&lt;봄꿩은 제 울음에 저 죽는다&gt;', '그 말과 같아서 사램이 잘못 되머넌 하는 ...</td>\n",
       "      <td>상처</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이야기꾼 구연설화</td>\n",
       "      <td>['그래 그 여자가 쪄서 쌀얼 쪄서 밥얼 했어.', '“잡수라.”구.', '그래 한...</td>\n",
       "      <td>상처</td>\n",
       "      <td>당황</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>이야기꾼 구연설화</td>\n",
       "      <td>['그랴. 그래 우리 인제 사춘 찾어간다는 얘기를 족~ 갈쳐중개,', '“그러시냐구...</td>\n",
       "      <td>상처</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>이야기꾼 구연설화</td>\n",
       "      <td>['나와서 인제 그 집 먼지 인저 그, 그, 뭐여 면사무소 있는 디 나와서 인제, ...</td>\n",
       "      <td>상처</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      title                                               text  \\\n",
       "0           0  이야기꾼 구연설화  ['01범보다 무서운 곶감', '화자를 처음 만나 이야기를 들으러 왔다고 하자 서슴...   \n",
       "1           1  이야기꾼 구연설화  ['<봄꿩은 제 울음에 저 죽는다>', '그 말과 같아서 사램이 잘못 되머넌 하는 ...   \n",
       "2           2  이야기꾼 구연설화  ['그래 그 여자가 쪄서 쌀얼 쪄서 밥얼 했어.', '“잡수라.”구.', '그래 한...   \n",
       "3           3  이야기꾼 구연설화  ['그랴. 그래 우리 인제 사춘 찾어간다는 얘기를 족~ 갈쳐중개,', '“그러시냐구...   \n",
       "4           4  이야기꾼 구연설화  ['나와서 인제 그 집 먼지 인저 그, 그, 뭐여 면사무소 있는 디 나와서 인제, ...   \n",
       "\n",
       "  top_sentiment second_sentiment  \n",
       "0            분노               당황  \n",
       "1            상처               분노  \n",
       "2            상처               당황  \n",
       "3            상처               분노  \n",
       "4            상처               분노  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_sentiment</th>\n",
       "      <th>second_sentiment</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>불안</td>\n",
       "      <td>당황</td>\n",
       "      <td>3 | 1 | “낫은 굽은 면을 다스리기 좋게 휘어 있지만 지금 너희는 그 장점을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>불안</td>\n",
       "      <td>상처</td>\n",
       "      <td>3 | 4 | \"물론 드래곤은 그녀의 양아버지야. 그리고 그녀를 대단히 사랑하고 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>불안</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>3 | 0 | 케이를 공항에 데려다 주었다는 말을 들으면 또 무슨 난리를 칠지 모른...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>불안</td>\n",
       "      <td>상처</td>\n",
       "      <td>3 | 4 | 방문객은 담담했다. 대신 품속에서 문서를 하나 꺼냈다. \"황제의 조서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>불안</td>\n",
       "      <td>당황</td>\n",
       "      <td>3 | 1 | \"이 여자의 남자가 건방지게스리 내 자리를 노리고 있다는구만!\" \"그...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     top_sentiment second_sentiment  \\\n",
       "157             불안               당황   \n",
       "3973            불안               상처   \n",
       "1018            불안               기쁨   \n",
       "1117            불안               상처   \n",
       "731             불안               당황   \n",
       "\n",
       "                                             input_text  \n",
       "157   3 | 1 | “낫은 굽은 면을 다스리기 좋게 휘어 있지만 지금 너희는 그 장점을 ...  \n",
       "3973  3 | 4 | \"물론 드래곤은 그녀의 양아버지야. 그리고 그녀를 대단히 사랑하고 있...  \n",
       "1018  3 | 0 | 케이를 공항에 데려다 주었다는 말을 들으면 또 무슨 난리를 칠지 모른...  \n",
       "1117  3 | 4 | 방문객은 담담했다. 대신 품속에서 문서를 하나 꺼냈다. \"황제의 조서...  \n",
       "731   3 | 1 | \"이 여자의 남자가 건방지게스리 내 자리를 노리고 있다는구만!\" \"그...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 감정 카테고리\n",
    "emotions = ['상처', '불안', '기쁨', '슬픔', '분노', '당황']\n",
    "\n",
    "# LabelEncoder 초기화\n",
    "le = LabelEncoder()\n",
    "le.fit(emotions)\n",
    "\n",
    "# 감정1과 감정2를 숫자로 변환\n",
    "data['감정1_encoded'] = le.transform(data['top_sentiment'])\n",
    "data['감정2_encoded'] = le.transform(data['second_sentiment'])\n",
    "\n",
    "# 수치형 감정1, 감정2와 텍스트 결합\n",
    "data['input_text'] = data['감정1_encoded'].astype(str) + \" | \" + data['감정2_encoded'].astype(str) + \" | \" + data['text']\n",
    "\n",
    "# 불필요한 문자 제거\n",
    "data['input_text'] = data['input_text'].str.replace(r\"[\\[\\],']\", \"\", regex=True)\n",
    "\n",
    "data[['top_sentiment', 'second_sentiment', 'input_text']].sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기쁨: 0\n",
      "당황: 1\n",
      "분노: 2\n",
      "불안: 3\n",
      "상처: 4\n",
      "슬픔: 5\n"
     ]
    }
   ],
   "source": [
    "# 감정별 숫자 매핑 출력\n",
    "for emotion, label in zip(le.classes_, le.transform(le.classes_)):\n",
    "    print(f\"{emotion}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class NovelsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, text_column=\"input_text\", max_length=128):\n",
    "        self.data = data.reset_index(drop=True)  # 인덱스 리셋 (중복 방지)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_column = text_column\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx, self.text_column] # input_text 가져오기\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        # labels 생성 (input_ids 복사 및 패딩 토큰을 -100으로 설정)\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# KOGPT2 모델과 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "# 모델 초기화\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NovelsDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_novels, eval_novels = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = NovelsDataset(train_novels, tokenizer)\n",
    "eval_dataset = NovelsDataset(eval_novels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer에 데이터셋 전달\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # 검증 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([ 9085,   739,   466, 10595,   739,   466, 10063,  8046,   406, 10401,\n",
      "         9198,  7895, 23775,   406,   377, 10063, 21319,  6889, 17272,  9269,\n",
      "        12679, 40422, 14636, 15255,  7769,  7048,  7285, 11218,   406,   377,\n",
      "         9054, 14669, 15728, 14243,  9860, 31427,  9342,  7610,  8711, 34692,\n",
      "        10769, 15255,  7769,  7048,  7162, 24444, 10351, 11993,  9137,  8563,\n",
      "         8705, 11688,  9712, 33930,  9661,  9399,  9769,  8359,  8704, 10652,\n",
      "        47980,  8811,  9071, 24444, 10185, 10644,   739,  7705,  8811, 12916,\n",
      "        22870,  7530,  9054, 18180,  9215,  7545, 27244,  9135, 13872, 10063,\n",
      "         8015,  9217,  7162, 11242,  9109,   389,  9093,  8529, 11727,  9040,\n",
      "        21713, 17044,  6897,  8146, 17885, 10704, 12205,  8711,   389,  9042,\n",
      "        25831, 17582, 36334, 33016,  9122,  9329,  9658,  8718,  7055,   406,\n",
      "          377, 10063,  8185, 29045,  9022,  6866, 13568, 47711, 10063, 43056,\n",
      "         9022,  7123,  7285, 14096,  9063,  9038, 11445, 12122])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Labels: tensor([ 9085,   739,   466, 10595,   739,   466, 10063,  8046,   406, 10401,\n",
      "         9198,  7895, 23775,   406,   377, 10063, 21319,  6889, 17272,  9269,\n",
      "        12679, 40422, 14636, 15255,  7769,  7048,  7285, 11218,   406,   377,\n",
      "         9054, 14669, 15728, 14243,  9860, 31427,  9342,  7610,  8711, 34692,\n",
      "        10769, 15255,  7769,  7048,  7162, 24444, 10351, 11993,  9137,  8563,\n",
      "         8705, 11688,  9712, 33930,  9661,  9399,  9769,  8359,  8704, 10652,\n",
      "        47980,  8811,  9071, 24444, 10185, 10644,   739,  7705,  8811, 12916,\n",
      "        22870,  7530,  9054, 18180,  9215,  7545, 27244,  9135, 13872, 10063,\n",
      "         8015,  9217,  7162, 11242,  9109,   389,  9093,  8529, 11727,  9040,\n",
      "        21713, 17044,  6897,  8146, 17885, 10704, 12205,  8711,   389,  9042,\n",
      "        25831, 17582, 36334, 33016,  9122,  9329,  9658,  8718,  7055,   406,\n",
      "          377, 10063,  8185, 29045,  9022,  6866, 13568, 47711, 10063, 43056,\n",
      "         9022,  7123,  7285, 14096,  9063,  9038, 11445, 12122])\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(\"Input IDs:\", sample['input_ids'])\n",
    "print(\"Attention Mask:\", sample['attention_mask'])\n",
    "print(\"Labels:\", sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2520' max='2520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2520/2520 10:58, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.206500</td>\n",
       "      <td>4.162277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.736700</td>\n",
       "      <td>4.153172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.402300</td>\n",
       "      <td>4.189591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.968600</td>\n",
       "      <td>4.243047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.769700</td>\n",
       "      <td>4.280783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2520, training_loss=3.4456768800341893, metrics={'train_runtime': 658.6562, 'train_samples_per_second': 30.646, 'train_steps_per_second': 3.826, 'total_flos': 1317434425344000.0, 'train_loss': 3.4456768800341893, 'epoch': 4.99851411589896})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 이야기 생성 함수\n",
    "def generate_story(lyrics_input, tag1=\"2\", tag2=\"4\"):\n",
    "    # 감정 태그를 앞에 추가하여 모델에 전달\n",
    "    emotion_input = f\"{tag1} | {tag2} | {lyrics_input}\"\n",
    "\n",
    "    # 입력 토큰화\n",
    "    input_ids = tokenizer.encode(emotion_input, return_tensors='pt').to(device)\n",
    "\n",
    "    # 출력 생성\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=150, # 새로 생성할 토큰의 개수를 제한\n",
    "        num_return_sequences=1, \n",
    "        temperature=0.8, \n",
    "        top_k=50, \n",
    "        top_p=0.9, \n",
    "        repetition_penalty=1.2, \n",
    "        do_sample=True  # 샘플링 활성화\n",
    "    )\n",
    "\n",
    "    # 입력 길이 추적\n",
    "    input_length = input_ids.shape[1]\n",
    "\n",
    "    # 생성된 토큰 중 입력 토큰 이후의 부분만 디코딩\n",
    "    generated_tokens = output[0][input_length:]\n",
    "    generated_story = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return generated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 이야기: ~ 라는 말을 꺼내지도 못하고 이러지 말끔히 털어놓고 눈 감고 잠잠깐 감춰둔 채 눈물만 하면 이제 그만 잠그려 그런 나를 속삭여 알았으니 그때를 그리워하는 건데 내가 평생 잊게 만든 내 인생이라는 걸 후회하며 톡 까마져 버릴 줄줄라 잔소리 한번 제대로 하지 못했었으면 좋겠네 하고 말이지 이렇게 말해 놓고 가만히 생각해 봐 이런 내가 한 번도 못해 너 때문에 참을 수 없었던 게 어쩌면 네 생각뿐이라며 왜 그랬더니... 라고 할 수만 있다면 진짜로 넌 정말 몰랐다면 정말 바보라는 말 한마디라도 해야 하는 거라며.... 이젠 더 이상 나의 마음을 어떻게 대처할까 고민하고 살았다고 해도 나 지금 사라질\n"
     ]
    }
   ],
   "source": [
    "# 가사로 이야기 생성\n",
    "lyrics_input = \"어쩌다 고작 그 마음도 못 참고 멍청하게 다 던졌는지 뭔가 들켜 버린 것 같아 표정을 보니 말이야 나도 티가 나버린 고백에 얼마나 놀랐는지 몰라 매일 치는 장난에도 두근댔고 오늘도 몇 번이고 떨렸지만 약속했어 날 안아줘 좀 알아줘 이건 꿈에서만 하기야 무심코 던진 니 말에 하루 종일 설레어 간직했다 아무도 못 보게 일기장에 적어 단단히 잠궜었는데 어쩌다 고작 그 마음도 못 참고 멍청하게 다 던졌는지 꾹꾹 참고 또 꼭꼭 숨겨서 이제까지 잘해 왔잖아 그러다 고작 울음도 못 참고 괜찮다 말하며 두 눈은 퉁퉁 붓고 코맹맹이가 되어도 난 내일은 맑음 예전처럼 옆에서 밥 먹어도 우연히 눈이 살짝 마주쳐도 걱정 마 날 안아줘 아니 사랑해줘 이건 꿈에서만 하니까 무심코 던진 니 말에 하루 종일 설레어 간직했다 아무도 못 보게 꼬깃꼬깃 구겨 씹어 다 삼켰었는데 어쩌다 고작 그 마음도 못 참고 멍청하게 다 던졌는지 꾹꾹 참고 또 꼭꼭 숨겨서 이제까지 잘 해 왔잖아 그러다 고작 울음도 못 참고 괜찮다 말하며 두 눈은 퉁퉁 붓고 코맹맹이가 되어도 난 사실 나 아주 오래 울 것 같아 고작 친구도 못 되니까 툭툭 털고 활짝 웃을 만큼 나는 그리 강하지가 않아 그러다 고작 사랑이 뭐라고 괜찮다 말하는 날까지 꾹꾹 참고 또 일기나 쓰고 있어 나 내 이름 맑음\"\n",
    "generated_story = generate_story(lyrics_input, tag1=\"2\", tag2=\"2\")\n",
    "print(\"생성된 이야기:\", generated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
